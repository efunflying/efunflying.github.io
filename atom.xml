<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>迎风飞翔</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://efunflying.github.io/"/>
  <updated>2017-08-12T15:12:14.000Z</updated>
  <id>https://efunflying.github.io/</id>
  
  <author>
    <name>Allen Fang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>(译)多核系统的软中断均衡</title>
    <link href="https://efunflying.github.io/2017/08/12/rps-softirq-load-balance/"/>
    <id>https://efunflying.github.io/2017/08/12/rps-softirq-load-balance/</id>
    <published>2017-08-12T08:53:42.000Z</published>
    <updated>2017-08-12T15:12:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近从QA那里听到一件尴尬事，我们正在评估的一台用于面向更低端市场的设备L，在防火墙性能上的表现，竟然比现有的那台最低端的设备B还要好。究其原因，是因为L虽然只是双核（B是4核），但是单核主频却比B更高，虽然之前同事针对多核做过优化，但他使用的优化策略是把网卡的软中断，依次一一绑定到固定的CPU上，如此一来，在单口进单口出的防火墙测试下，更多的核就没有优势了，反而这个时候主频变成了决定性的因素。听到这个消息的第一印象就是这样的优化方案存在问题，如果可以把全局的软中断分布到各个CPU上，那么这个问题，也许就不存在了。</p>
<p>放狗一搜，找到一篇不错的文章，顺手翻译一下，看起来对于目前产品遇到的这个问题，是一剂良方。</p>
<p>=================================译文分割线==================================</p>
<p>Some years ago I have tested network interrupts affinity - you set ~0 as a CPU mask to balance network interrupts among all your CPU cores and you get all softirq instances running in parallel. Such interrupts distribution among CPU cores sometimes is a bad idea due to CPU caches computational burden and probable packets reordering. In most cases it is not recommended for servers performing some TCP application (e.g. web server). However this ability is crucial for some low level packet applications like firewalls, routers or Anti-DDoS solutions (in last cases most of the packets must be dropped as quick as possible), which do a lot of work in softirq. So for some time I was thinking that there is no problem to share softirq load between CPU cores.</p>
<p>几年前我测试过网络中断亲缘性 — 设置 ~0为CPU掩码来在所有CPU核间平衡网络中断，这样就可以让所有的软中断实例并行的执行。在CPU核间这样的中断分布有时候并不是一个好主意，因为CPU本来有cache计算负荷的能力，以及还有可能发生包的乱序。在多数情况下对于运行TCP应用的服务器（比如说Web服务器），并不建议这样做。然而此能力对于一些进行大量软中断处理的底层包应用，如防火墙，路由器以及Anti-DDos方案(要求对大部分的包尽快的drop掉)来说是至关重要的。所以一度，我以为在CPU核间共享软中断是可行的。</p>
<p>To get softirq sharing between CPU cores you just need to do</p>
<p>为了让软中断在CPU核间共享，你只需要如此：</p>
<pre><code>$ for irq in `grep eth0 /proc/interrupts | cut -d: -f1`; do \
    echo ffffff &gt; /proc/irq/$irq/smp_affinity; \
done
</code></pre><p>This makes (as I thought) your APIC to distribute interrupts between all your CPUs in round-robin fashion (or probably using some more cleaver technique). And this really was working in my tests.</p>
<p>这使你的APIC将中断以round-robin(也可能用某些更聪明的技术)的方式分布到所有的CPU上。并且在我的测试中，也是正常工作的。</p>
<p>Recently our client concerned about this ability, so I wrote very simple testing kernel module which just makes more work in softirq:</p>
<p>最近我们的客户对这个能力有concern，所以我写了个很简单的测试内核模块，这些模块仅仅用来产生更多的软中断工作：</p>
<p>#include</p>
<p>#include</p>
<p>#include</p>
<p>#include</p>
<p>MODULE_LICENSE(“GPL”);</p>
<p>/**</p>
<ul>
<li>Just eat some local CPU time and accept the packet.<br><em>/<br>static unsigned int<br>st_hook(unsigned int hooknum, struct sk_buff </em>skb,<pre><code>const struct net_device *in,
</code></pre>const struct net_device *out,<pre><code>int (*okfn)(struct sk_buff *))
</code></pre>{<br>unsigned int i;<br>for (i = 0; i &lt;= 1000 * 1000; ++i)<br>skb_linearize(skb);</li>
</ul>
<p>return NF_ACCEPT;<br>}</p>
<p>static struct nf_hook_ops st_ip_ops[] __read_mostly = {<br>{<br>.hook = st_hook,<br>.owner = THIS_MODULE,<br>.pf = PF_INET,<br>.hooknum = NF_INET_PRE_ROUTING,<br>.priority = NF_IP_PRI_FIRST,<br>},<br>};</p>
<p>static int <strong>init<br>st_init(void)<br>{<br>if (nf_register_hooks(st_ip_ops, ARRAY_SIZE(st_ip_ops))) {<br>printk(KERN_ERR “%s: can’t register nf hook\n”,
</strong>FILE<strong>);<br>return1;<br>}<br>printk(KERN_ERR “%s: loaded\n”, </strong>FILE__);</p>
<p>return0;<br>}</p>
<p>static void<br>st_exit(void)<br>{<br>    nf_unregister_hooks(st_ip_ops, ARRAY_SIZE(st_ip_ops));<br>    printk(KERN_ERR “%s: unloaded\n”, <strong>FILE</strong>);<br>}</p>
<p>module_init(st_init);<br>module_exit(st_exit);</p>
<p>I loaded the system with iperf over 1Gbps channel. And I was very confused when see that only one CPU of 24-cores machine was doing whole the work and all other CPUs was doing nothing!</p>
<p>我在系统中起了iperf，并且加上了每条channel 1Gbps的流量。当我看到24颗CPU中只有一颗在做所有的工作，而其他CPU什么都不干时，我觉得非常困惑。</p>
<p>To understand what’s going on lets have a look how Linux handles incoming packets and interrupts from network card (e.g. Intel 10 Gigabit PCI Express which is placed at drivers/net/ixgbe). Softirq works in per-cpu kernel threads, ksoftirqd (kernel/softirq.c:ksoftirqd()), i.e. if you have 4-cores machine, then you have 4 ksoftirqd threads (ksoftirqd/0, ksoftirqd/1, ksoftirqd/2 and ksoftirqd/3). ksoftirqd() calls do_softirq(), which by-turn calls <strong>do_softirq(). The last one uses softirq_vec vector to get required hadler for current softirq type (e.g. NET_RX_SOFTIRQ for receiving or NET_TX_SOFTIRQ for sending softirqs correspondingly).The next step is to call virtual function action() for the handler. For NET_RX_SOFTIRQ net_rx_action() (net/core/dev.c) is called here.net_rx_action() reads napi_struct from per-cpu queue softnet_data and calls virtual function poll() - a NAPI callback (ixgbe_poll() in our case) which actually reads packets from the device ring queues.The driver processes interrupts in ixgbe_intr(). This function runs NAPI through call </strong>napi_schedule(),which pushes current napi_struct to per-cpu softnet_data-&gt;poll_list, which net_rx_action() reads packets (on the same CPU) from. Thus softirq runs on the same core which received hardware interrupt.</p>
<p>为了弄清楚发生了什么，我们来看一下Linux是怎样处理incoming的包，以及网卡产生的中断的(比如说，位于driver/net/ixgbe的Intel 10Gigabit PCI Express)。软中断工作于每CPU一个的内核线程中，ksoftirqd(kernel/softirq.c:ksoftirqd())，比方说你有一台4核的机器，那你就有4个ksoftirqd的线程(ksoftirqd/0, ksoftirqd/1, ksoftirqd/2 and ksoftirqd/3)。ksoftirqd()调用do_softirq()，后者又进一步调用<strong>do_softirq()，</strong>do_softirq()利用softirq_vec向量来获取当前软中断类型所需要的handler（如用于接收软中断的NET_RX_SOFTIRQ或用于发送软中断的NET_TX_SOFTIRQ）。接下去的一部是handler调用虚函数action()。对于NET_RX_SOFTIRQ来说，net/core/dev.c中的net_rx_action()就是在这里被调用的。net_rx_action()从每CPU一个的queue softnet_data中读取napi_struct以及调用虚函数poll() —— 一个NAPI callback（在我们这个case中是ixgbe_poll()），这个函数真正的从设备ring queue中读入包。驱动在ixgbe_intr()中处理中断，此函数通过调用<strong>napi_schedule()运行NAPI，</strong>napi_schedule将当前的napi_struct推入每CPU一个的softnet_data-&gt;poll_list，正是从这里，同一个CPU上的net_rx_action()读取包。因此软中断跑在收到硬件中断的同一个核上。</p>
<p>This way theoretically if harware interrupts are going to N cores, then these and only these N cores are doing softirq. So I had a look at /proc/interrupts statistics and saw that only one 0th core is actually receiving interrupts from NIC while I set ~0 mask in smp_affinity for the interupt (actually I had MSI-X card, so I set the mask to all the interrupt vectors for the card).</p>
<p>这样一来，理论上如果硬件中断被分去N个核，有且仅有这N个核会去执行软中断。所以我看了一下/proc/interrupts，发现当我为中断设置~0的掩码于smp_affinity时（实际上我有一块MSI-X的卡，所以我为这张卡的所有中断向量设置了掩码），事实上只有第0号CPU在接收网卡上的中断。</p>
<p>I started googling for the answers why on earth interupts do not distribute among all the cores. The first topics which I found were nice articles by Alexander Sandler:<br>我开始google，到底为什么中断没有分不到所有的核。我首先找到的几个topic，是由Alexander Sandler写的几篇好文：<br>SMP affinity and proper interrupt handling in Linux<br>Why interrupt affinity with multiple cores is not such a good thing<br>MSI-X – the right way to spread interrupt load</p>
<p>Following these articles not all hardware is actually able to spread interrupts between CPU cores. During my tests I was using IBM servers of particular model, but this is not the case of the client - they use very different hardware. This is why I saw one nice picture on my previous tests, but faced quite different behaviour on other hardware.</p>
<p>按照这些文章，不是所有的硬件都有能力将中断分布到CPU的各个核上。在我的测试中，我使用了某个特定型号的IBM Server，但是我的客户不是这样，他们使用了很不一样的硬件。这就是为什么我在我先前的测试中看到了美妙的画面，而在其他的硬件上看到了截然不同的行为。</p>
<p>The good news is that linux 2.6.35 has introduced nice feature -  RPS (Receive Packet Steering). The core of the feature is get_rps_cpu() from dev/net/core.c, which computes a hash from IP source and destination addresses of an incoming packet and determines a which CPU send the packet to based on the hash. netif_receive_skb() or netif_rx() which call the function puts the packet to appropriate per-cpu queue for further processing by softirq. So there are two important consequences:<br>好消息是，在linux 2.6.35中引入了很棒的feature——RPS（Receive Packet Steering）。这个feature的核心就是/dev/net/core.c中的get_rps_cpu()，这个函数由incoming包的SRC IP和DST IP计算出一个hash值，并且基于这个hash值，确定把包发送给那个CPU。netif_receive_skb()或者netif_rx()，它们调用此函数将包放入合适的，每CPU一个的queue中，这些包将在未来就地(CPU)由软中断处理。所以两个重要的效果是：<br>packets are processed by different CPUs (with processing I mostly mean Netfilter pre-routing hooks);</p>
<pre><code>1. 包由不同的CPU处理（这里的处理，我通常值Netfilter的pre-routing hooks）
</code></pre><p>it is unlikely that packets belonging to the same TCP stream are reordered (packets reordering is a well-known problem for TCP performance, see for example Beyond softnet).</p>
<pre><code>2. 属于同一个TCP流的包不太可能发生乱序（包乱序一直是影响TCP性能的一个著名问题，这一点可以参考Beyond softnet的例子）。
</code></pre><p>To enable the feature you should specify CPUs mask as following (the adapter from the example is connected via MSI-X and has 8 tx-rx queues, so we need to update masks for all the queues):<br>要启用这一feature，你需要像下面这样指定CPU的掩码（此例中的网卡通过MSI-X连接，并有8个tx-rx队列，所以我们需要update所有这些队列的掩码）<br>    $ for i in <code>seq 0 7</code>; do \<br>        echo fffffff &gt; /sys/class/net/eth0/queues/rx-$i/rps_cpus ; \<br>    done</p>
<p>After running linux-2.6.35 and setting all CPUs to be able to process softirq I got following nice picture in top:<br>在运行linux-2.6.35以及设置所有CPU都能处理软中断后，我得到了top里可以看到的如下不错的结果：</p>
<p>  2238 root      20   0  411m  888  740 S  152  0.0   2:38.94 iperf<br>    10 root      20   0     0    0    0 R  100  0.0   0:35.44 ksoftirqd/2<br>    19 root      20   0     0    0    0 R  100  0.0   0:46.48 ksoftirqd/5<br>    22 root      20   0     0    0    0 R  100  0.0   0:29.10 ksoftirqd/6<br>    25 root      20   0     0    0    0 R  100  0.0   2:47.36 ksoftirqd/7<br>    28 root      20   0     0    0    0 R  100  0.0   0:33.73 ksoftirqd/8<br>    31 root      20   0     0    0    0 R  100  0.0   0:46.63 ksoftirqd/9<br>    40 root      20   0     0    0    0 R  100  0.0   0:45.33 ksoftirqd/12<br>    46 root      20   0     0    0    0 R  100  0.0   0:29.10 ksoftirqd/14<br>    49 root      20   0     0    0    0 R  100  0.0   0:47.35 ksoftirqd/15<br>    52 root      20   0     0    0    0 R  100  0.0   2:33.74 ksoftirqd/16<br>    55 root      20   0     0    0    0 R  100  0.0   0:46.92 ksoftirqd/17<br>    58 root      20   0     0    0    0 R  100  0.0   0:32.07 ksoftirqd/18<br>    67 root      20   0     0    0    0 R  100  0.0   0:46.63 ksoftirqd/21<br>    70 root      20   0     0    0    0 R  100  0.0   0:28.95 ksoftirqd/22<br>    73 root      20   0     0    0    0 R  100  0.0   0:45.03 ksoftirqd/23<br>     7 root      20   0     0    0    0 R   99  0.0   0:47.97 ksoftirqd/1<br>    37 root      20   0     0    0    0 R   99  0.0   2:42.29 ksoftirqd/11<br>    34 root      20   0     0    0    0 R   77  0.0   0:28.78 ksoftirqd/10<br>    64 root      20   0     0    0    0 R   76  0.0   0:30.34 ksoftirqd/20</p>
<p>So as we see almost all of the cores are doing softirqs.<br>正如我们看到的，几乎所有的core都在处理软中断。</p>
<p>以下是原文：</p>
<blockquote>
<h1 id="scaling-softirq-among-many-CPU-cores"><a href="#scaling-softirq-among-many-CPU-cores" class="headerlink" title="scaling softirq among many CPU cores"></a>scaling softirq among many CPU cores</h1><p>Some years ago I have tested network interrupts affinity - you set ~0 as a CPU mask to balance network interrupts among all your CPU cores and you get all softirq instances running in parallel. Such interrupts distribution among CPU cores sometimes is a bad idea due to CPU caches computational burden and probable packets reordering. In most cases it is not recommended for servers performing some TCP application (e.g. web server). However this ability is crucial for some low level packet applications like firewalls, routers or Anti-DDoS solutions (in last cases most of the packets must be dropped as quick as possible), which do a lot of work in softirq. So for some time I was thinking that there is no problem to share softirq load between CPU cores.</p>
<p>To get softirq sharing between CPU cores you just need to do</p>
<pre><code>$ for irq in `grep eth0 /proc/interrupts | cut -d: -f1`; do \
    echo ffffff &gt; /proc/irq/$irq/smp_affinity; \
done
</code></pre><p>This makes (as I thought) your APIC to distribute interrupts between all your CPUs in round-robin fashion (or probably using some more cleaver technique). And this really was working in my tests.</p>
<p>Recently our client concerned about this ability, so I wrote very simple testing kernel module which just makes more work in softirq:</p>
</blockquote>
<pre><code>#include &lt;linux/kernel.h&gt;
#include &lt;linux/module.h&gt;
#include &lt;linux/netfilter.h&gt;
#include &lt;linux/netfilter_ipv4.h&gt;

MODULE_LICENSE(&quot;GPL&quot;);

/**
 * Just eat some local CPU time and accept the packet.
 */
static unsigned int st_hook(unsigned int hooknum, struct sk_buff *skb,
        const struct net_device *in,
        const struct net_device *out,
        int (*okfn)(struct sk_buff *))
{
    unsigned int i;
    for (i = 0; i &lt;= 1000 * 1000; ++i)
        skb_linearize(skb);

    return NF_ACCEPT;
}

static struct nf_hook_ops st_ip_ops[] __read_mostly = {
    {
        .hook = st_hook,
        .owner = THIS_MODULE,
        .pf = PF_INET,
        .hooknum = NF_INET_PRE_ROUTING,
        .priority = NF_IP_PRI_FIRST,
    },
};

static int __init st_init(void)
{
    if (nf_register_hooks(st_ip_ops, ARRAY_SIZE(st_ip_ops))) {
        printk(KERN_ERR &quot;%s: can&apos;t register nf hook\n&quot;,
                __FILE__);
        return1;
    }
    printk(KERN_ERR &quot;%s: loaded\n&quot;, __FILE__);

    return0;
}

static void st_exit(void)
{
    nf_unregister_hooks(st_ip_ops, ARRAY_SIZE(st_ip_ops));
    printk(KERN_ERR &quot;%s: unloaded\n&quot;, __FILE__);
}

module_init(st_init);
module_exit(st_exit);
</code></pre><blockquote>
<p>I loaded the system with iperf over 1Gbps channel. And I was very confused when see that only one CPU of 24-cores machine was doing whole the work and all other CPUs was doing nothing!</p>
<p>To understand what’s going on lets have a look how Linux handles incoming packets and interrupts from network card (e.g. Intel 10 Gigabit PCI Express which is placed at drivers/net/ixgbe). Softirq works in per-cpu kernel threads, ksoftirqd (kernel/softirq.c:ksoftirqd()), i.e. if you have 4-cores machine, then you have 4 ksoftirqd threads (ksoftirqd/0, ksoftirqd/1, ksoftirqd/2 and ksoftirqd/3). ksoftirqd() calls do_softirq(), which by-turn calls <strong>do_softirq(). The last one uses softirq_vec vector to get required hadler for current softirq type (e.g. NET_RX_SOFTIRQ for receiving or NET_TX_SOFTIRQ for sending softirqs correspondingly).The next step is to call virtual function action() for the handler. For NET_RX_SOFTIRQ net_rx_action() (net/core/dev.c) is called here.net_rx_action() reads napi_struct from per-cpu queue softnet_data and calls virtual function poll() - a NAPI callback (ixgbe_poll() in our case) which actually reads packets from the device ring queues.The driver processes interrupts in ixgbe_intr(). This function runs NAPI through call </strong>napi_schedule(),which pushes current napi_struct to per-cpu softnet_data-&gt;poll_list, which net_rx_action() reads packets (on the same CPU) from. Thus softirq runs on the same core which received hardware interrupt.</p>
<p>This way theoretically if harware interrupts are going to N cores, then these and only these N cores are doing softirq. So I had a look at /proc/interrupts statistics and saw that only one 0th core is actually receiving interrupts from NIC while I set ~0 mask in smp_affinity for the interupt (actually I had MSI-X card, so I set the mask to all the interrupt vectors for the card).</p>
<p>I started googling for the answers why on earth interupts do not distribute among all the cores. The first topics which I found were nice articles by Alexander Sandler:<br>SMP affinity and proper interrupt handling in Linux<br>Why interrupt affinity with multiple cores is not such a good thing<br>MSI-X – the right way to spread interrupt load</p>
<p>Following these articles not all hardware is actually able to spread interrupts between CPU cores. During my tests I was using IBM servers of particular model, but this is not the case of the client - they use very different hardware. This is why I saw one nice picture on my previous tests, but faced quite different behaviour on other hardware.</p>
<p>The good news is that linux 2.6.35 has introduced nice feature -  RPS (Receive Packet Steering). The core of the feature is get_rps_cpu() from dev/net/core.c, which computes a hash from IP source and destination addresses of an incoming packet and determines a which CPU send the packet to based on the hash. netif_receive_skb() or netif_rx() which call the function puts the packet to appropriate per-cpu queue for further processing by softirq. So there are two important consequences:</p>
<pre><code>1. packets are processed by different CPUs (with processing I mostly mean Netfilter pre-routing hooks);
2. it is unlikely that packets belonging to the same TCP stream are reordered (packets reordering is a well-known problem for TCP performance, see for example Beyond softnet).
</code></pre><p>To enable the feature you should specify CPUs mask as following (the adapter from the example is connected via MSI-X and has 8 tx-rx queues, so we need to update masks for all the queues):<br>    $ for i in <code>seq 0 7</code>; do \<br>        echo fffffff &gt; /sys/class/net/eth0/queues/rx-$i/rps_cpus ; \<br>    done</p>
<p>After running linux-2.6.35 and setting all CPUs to be able to process softirq I got following nice picture in top:</p>
<p>  2238 root      20   0  411m  888  740 S  152  0.0   2:38.94 iperf<br>    10 root      20   0     0    0    0 R  100  0.0   0:35.44 ksoftirqd/2<br>    19 root      20   0     0    0    0 R  100  0.0   0:46.48 ksoftirqd/5<br>    22 root      20   0     0    0    0 R  100  0.0   0:29.10 ksoftirqd/6<br>    25 root      20   0     0    0    0 R  100  0.0   2:47.36 ksoftirqd/7<br>    28 root      20   0     0    0    0 R  100  0.0   0:33.73 ksoftirqd/8<br>    31 root      20   0     0    0    0 R  100  0.0   0:46.63 ksoftirqd/9<br>    40 root      20   0     0    0    0 R  100  0.0   0:45.33 ksoftirqd/12<br>    46 root      20   0     0    0    0 R  100  0.0   0:29.10 ksoftirqd/14<br>    49 root      20   0     0    0    0 R  100  0.0   0:47.35 ksoftirqd/15<br>    52 root      20   0     0    0    0 R  100  0.0   2:33.74 ksoftirqd/16<br>    55 root      20   0     0    0    0 R  100  0.0   0:46.92 ksoftirqd/17<br>    58 root      20   0     0    0    0 R  100  0.0   0:32.07 ksoftirqd/18<br>    67 root      20   0     0    0    0 R  100  0.0   0:46.63 ksoftirqd/21<br>    70 root      20   0     0    0    0 R  100  0.0   0:28.95 ksoftirqd/22<br>    73 root      20   0     0    0    0 R  100  0.0   0:45.03 ksoftirqd/23<br>     7 root      20   0     0    0    0 R   99  0.0   0:47.97 ksoftirqd/1<br>    37 root      20   0     0    0    0 R   99  0.0   2:42.29 ksoftirqd/11<br>    34 root      20   0     0    0    0 R   77  0.0   0:28.78 ksoftirqd/10<br>    64 root      20   0     0    0    0 R   76  0.0   0:30.34 ksoftirqd/20</p>
<p>So as we see almost all of the cores are doing softirqs.</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近从QA那里听到一件尴尬事，我们正在评估的一台用于面向更低端市场的设备L，在防火墙性能上的表现，竟然比现有的那台最低端的设备B还要好。究其原因，是因为L虽然只是双核（B是4核），但是单核主频却比B更高，虽然之前同事针对多核做过优化，但他使用的优化策略是把网卡的软中断，依次
    
    </summary>
    
    
      <category term="kernel" scheme="https://efunflying.github.io/tags/kernel/"/>
    
      <category term="TCP/IP" scheme="https://efunflying.github.io/tags/TCP-IP/"/>
    
  </entry>
  
  <entry>
    <title>golang的二三事</title>
    <link href="https://efunflying.github.io/2017/08/07/golang-notes/"/>
    <id>https://efunflying.github.io/2017/08/07/golang-notes/</id>
    <published>2017-08-06T16:56:49.000Z</published>
    <updated>2017-08-06T17:16:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近闲暇时间会看一些golang的东西，顺便开篇博，以便记录所思所得。一开始必然会是零碎的点，希望丰富之后能重新组织成体系完备的文章。</p>
<ol>
<li>go build helloworld.go默认是静态编译，动态编译的话，需要加上参数-buildmode=c-shared，有趣的是，在系统没有安装libc-static的情况下，默认命令依然生成了静态编译的代码。</li>
<li><p>go编译出的代码天生就引用pthread.so</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Golang版本的helloworld:</div><div class="line">[root@localhost go]# ldd helloworld</div><div class="line">ldd: warning: you do not have execution permission for `./helloworld&apos;</div><div class="line">        linux-gate.so.1 =&gt;  (0x0043f000)</div><div class="line">        libpthread.so.0 =&gt; /lib/libpthread.so.0 (0x0021b000)</div><div class="line">        libc.so.6 =&gt; /lib/libc.so.6 (0x006a6000)</div><div class="line">        /lib/ld-linux.so.2 (0x0049e000)</div><div class="line">而C版本的hello(gcc -o hello -static hello.c生成，需要安装libc-static):</div><div class="line">[root@localhost go]# ldd hello</div><div class="line">        linux-gate.so.1 =&gt;  (0x0061c000)</div><div class="line">        libc.so.6 =&gt; /lib/libc.so.6 (0x003d5000)</div><div class="line">        /lib/ld-linux.so.2 (0x00ea9000)</div></pre></td></tr></table></figure>
</li>
<li><p>closure的意义是对于其参数而言的，这个变量实现了C/C++中的static类似的语义，即限定范围的全局变量。</p>
</li>
<li>defer的哲学大概是“方便管理”，用于改进传统的语言中，创建和释放由于执行顺序往往需要离得很远，在维护时容易疏漏的“缺点”。一个典型的应用场景也许是“锁”，defer的声明会自动维护反向释放的效果（栈式的调用），而且如果always是在函数返回前调用的话，那么可以避免一些由于错误处理的逻辑比较复杂，而不小心造成某些分支忘记释放资源的错误。相比在各个分支上加资源释放的代码而言，这种实现更加“优雅”，这个意义上有点类似于C中常常用goto实现的功能，但如前所说，由于写完申请就写释放，检查配对会比较容易。</li>
<li>(to be continued…)</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近闲暇时间会看一些golang的东西，顺便开篇博，以便记录所思所得。一开始必然会是零碎的点，希望丰富之后能重新组织成体系完备的文章。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;go build helloworld.go默认是静态编译，动态编译的话，需要加上参数-buildmode=c-
    
    </summary>
    
    
      <category term="golang" scheme="https://efunflying.github.io/tags/golang/"/>
    
  </entry>
  
  <entry>
    <title>一些不常用，但关键时候很有用的Linux命令知识</title>
    <link href="https://efunflying.github.io/2017/07/31/linux-command-collection/"/>
    <id>https://efunflying.github.io/2017/07/31/linux-command-collection/</id>
    <published>2017-07-30T17:33:01.000Z</published>
    <updated>2017-08-06T14:00:33.000Z</updated>
    
    <content type="html"><![CDATA[<ol>
<li><p>paste + 进程替换<br>当你想要把几个命令的输出结果合并起来，作为后面命令的输入时，会需要这个命令。<br>e.g. 查找当前目录下所有的c，h以及cpp文件，并传给calltree当参数<br>paste &lt;(find . -name “<em>.[ch]”) &lt;(find . -name “</em>.cpp”) | xargs calltree -bg list=”func”<br>注1：&lt;与圆括号之间没有空格<br>注2：xargs cmd与xargs -I”{}” cmd {}的区别在于xargs默认会把输入的换行符换成空格，命令执行一次，参数是所有的输入内容用空格隔开，而-I参数则是用输入的内容做替换，但同时并不会把输入中的换行换成空格，即同一个命令执行多次，每次用一行的参数<br>注3：实践证明calltree是个坑，久未更新，一些结果已经很不准确，cflow看起来不错，有不断的维护，ascii图适合简单的写文章场景，结合吴章京同学的脚本，可以转成dot文件供graphviz生成图片文件。</p>
</li>
<li><p>上传某个文件到ftp（改变命令序列可实现不同功能）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">cd /PATH_YOU_WANT_TO_UPLOAD(DOWNLOAD)  </div><div class="line">ftp -niv &lt;&lt;- EOF  </div><div class="line">open IP_ADDRESS  </div><div class="line">user USERNAME PASSWORD  </div><div class="line">ascii(or bin)  </div><div class="line">put *(or get)  </div><div class="line">bye  </div><div class="line">EOF</div></pre></td></tr></table></figure>
</li>
</ol>
<p>注：关于ftp -niv &lt;&lt;- EOF<br>由于是脚本，因此我们要给此FTP脚本定义一个结束的符号，在这里，定义的结束符是“EOF”。你也可以自己来定义其他的。不过建议用“EOF”，这在绝大多数编程语言中，都表示结束：End Of File。<br>-v：显示远程服务器的所有响应信息；<br>-n：限制FTP的自动登录，即不使用；<br>-i：关闭多个文件传输时的交互过程；</p>
]]></content>
    
    <summary type="html">
    
      &lt;ol&gt;
&lt;li&gt;&lt;p&gt;paste + 进程替换&lt;br&gt;当你想要把几个命令的输出结果合并起来，作为后面命令的输入时，会需要这个命令。&lt;br&gt;e.g. 查找当前目录下所有的c，h以及cpp文件，并传给calltree当参数&lt;br&gt;paste &amp;lt;(find . -name “&lt;
    
    </summary>
    
    
      <category term="linux" scheme="https://efunflying.github.io/tags/linux/"/>
    
      <category term="shell" scheme="https://efunflying.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>ip_early_demux魅影</title>
    <link href="https://efunflying.github.io/2017/07/30/ip-early-demux/"/>
    <id>https://efunflying.github.io/2017/07/30/ip-early-demux/</id>
    <published>2017-07-30T08:52:58.000Z</published>
    <updated>2017-07-30T13:00:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>上周终于把maninmid的事搞定了，其实回头来说，关于最关键的问题，很早就已经想到最后的那个方向了。但由于用户态daemon打印的log产生了误导，然后加上porting过程当中本来也产生了一些其他的疏漏，结果最后差不多把TCP/IP协议栈过了一遍，顺便修了几个porting的问题，确认了其他部分没问题之后，才重新回到了最初怀疑的方向，可谓一波三折。</p>
<p>我们实现maninmid的思路和tproxy很类似，也是利用connection track来管理session的mark，然后根据包的mark来确定其路由策略，不过由于我们对kernel进行了一些hack，这让我们做maninmid的时机时机和策略比使用iptables+tproxy更灵活，针对性也更精准，可以在我们在识别到某个特定的traffic类型的时候，在会话的中途注入进去。</p>
<p>不过这次在从2.6.32 porting到3.10的时候，这个patch的正常运行出了问题，在排除了所有poritng过程中的疏漏后，man in middle依然不工作。</p>
<p>一个奇怪的现象是，根据man in middle的逻辑，我们修改了ip_route_input_slow，所有打上了mark的包在做路由前，都会被我们添加于此的逻辑直接导向local in，转去ip_local_deliver处理，但是实际的情况，根据systemtap的显示，有相当一部分的包进入了forwarding chain，也就是去了ip_forward，这让人非常费解。这些被打上了标记的包，最终由netfilter queue导到用户态daemon，被我们认为是异常包丢弃了。同时期的抓包发现，一方面，客户端在某个阶段后，对于server(由于是man in middle，所以这个server其实是我们)发来的某个包不断的重发ACK，另一方面，我们却好像没有收到这些ack包一样，对于之前的那个包也在反复重传。看起来，那些进入forwarding chain被丢掉的包，和client端发送的那些ACK包很像是同一批的包。</p>
<p>那么那些包怎么会跑到了forwarding chain上去了呢，经过代码追踪，最终怀疑到了input_rcv_finish函数中引用的一个名为sysctl_ip_early_demux的变量，由于这个名字看起来是可以通过sysctl干预的，在/proc目录下一查，果然存在这么个文件/proc/sys/net/ipv4/ip_early_demux，并且值为1，将其改为0后，man in middle的逻辑终于正常了。</p>
<p>那么这个ip_early_demux到底是个什么参数呢，根据kernel document里的描述，</p>
<blockquote>
<p>ip_early_demux - BOOLEAN</p>
<pre><code>Optimize input packet processing down to one demux for
certain kinds of local sockets.  Currently we only do this
for established TCP and connected UDP sockets.

It may add an additional cost for pure routing workloads that
reduces overall throughput, in such case you should disable it.
Default: 1  
</code></pre></blockquote>
<p>其实现大体就是在socket(sock)里缓存了路由的结果(dst)，一个包到达的时候，先去找这个包有没有对应的socket，如果有的话，看看socket里的dst存不存在，如果存在的话就直接给置了包对应的dst_entry，并且由此绕过了后面的路由代码(因为不需要了)。</p>
<p>这样一来就好解释了，由于我们是在中途inject进去的，所以建立socket的时候，缓存的路由依然是做转发的，因此在这之后，对于客户端发来的ack包，由于一开始就查到了socket中dst_entry，所以就绕开了路由的代码，也就绕开了我们将它们直接转向本地的代码，而去了缓存下来的转发流程，由于在prerouting的时候被打上了mark，所以他们在之后被当做不该出现的包被丢掉了。所以作为上层的socket来说，始终收不到客户端发来的ack包，所以就不断重发，而客户端也因此不断的继续回ack包，知道最终上层超时，连接失败。</p>
<p>由于如文档所说，这个属性因为上来不管包是不是到本地都会去查找相应的socket，因此对于转发为主的网络设备其实是不利(大多数情况下会失败，类似分支预测失误)的，所以最终方案打算直接禁掉这个功能。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上周终于把maninmid的事搞定了，其实回头来说，关于最关键的问题，很早就已经想到最后的那个方向了。但由于用户态daemon打印的log产生了误导，然后加上porting过程当中本来也产生了一些其他的疏漏，结果最后差不多把TCP/IP协议栈过了一遍，顺便修了几个porti
    
    </summary>
    
    
      <category term="kernel" scheme="https://efunflying.github.io/tags/kernel/"/>
    
      <category term="TCP/IP" scheme="https://efunflying.github.io/tags/TCP-IP/"/>
    
  </entry>
  
  <entry>
    <title>(译)修复TCP连接</title>
    <link href="https://efunflying.github.io/2017/07/19/repair-tcp-connection/"/>
    <id>https://efunflying.github.io/2017/07/19/repair-tcp-connection/</id>
    <published>2017-07-19T14:07:04.000Z</published>
    <updated>2017-07-19T19:33:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>这是2012年的一篇老文章了，最近在做产品kernel base upgrading的事，从2.6.32到3.10的迁移过程中，原来在kernel中实现man in middle支持的patch在porting到3.10后出了一点问题，为maninmid建立的新socket工作不正常，看起来和新的kernel改动有关。在排查问题的时候偶然发现了这篇文章，觉得有了TCP_REPAIR这种模式的支持，原来的代码看起来可以实现得更加优雅，虽然不确定最终是否会采用这种方案，因为它会需要修改用户态daemon的实现，不知道领导是否有意见，但从技术上，觉得这是个好方向。</p>
<p>=================================译文分割线==================================</p>
<p>把一个正在运行的container从一台物理主机迁移到另一台物理主机是件在许多层面上都很tricky的事。尤其当container拥有许多与外部的active网络连接的时候，问题就变得更加困难。一个自然的想法是希望这些连接能够随着container迁移到新的物理主机上，而且最好远端都对此过程毫无察觉，不过Linux网络栈在编写是并没有考虑这样的移动的。即使这样，似乎随着Pavel Emelyanov的TCP connection repair patches，网络连接的transparent relocation，将在3.5版本的内核上得到支持。</p>
<p>迁移TCP连接的第一步是收集该连接当前所有可能的信息。这些信息目前在用户态很多都能获取到。通过挖掘/proc和/sys，可以确定对端的地址和端口，发送和接收队列的大小，TCP的sequence number，以及两端协商而得到的一堆参数。当然仍然有一些参数是用户态获取不到的，所以为了完成这个任务，内核也需要提供一些额外的支持。</p>
<p>对于拥有适当特权（CAP_NET_ADMIN）的进程，Pavel的patch提供了这样的支持。为了深入一个active的网络连接的内部，用户空间需要使用新引入的TCP_REPAIR选项，通过setsocketopt()系统调用，将相关的socket置于“修复模式(repair mode)”下，并且此时的socket必须处于关闭或者“established”状态下。一旦socket被置于修复模式，它就能以数种方式进行操控。</p>
<p>其中一种方式是读取发送和接受队列的内容。发送队列包含了还没有成功发送到对端的数据，这些数据需要随着连接一起迁移，这样它们就能在新的机器上被继续发送。接受队列包含了已从对端接收到，但是还没有来得及被即将迁移的应用程序处理的数据，它们也应该迁移过去，以供到了新的机器上被迁移过去的应用程序读取。获取这两个队列的内容是通过两个步骤完成的：（1）调用setsockopt(TCP_REPAIR_QUEUE)，带上参数TCP_RECV_QUEUE或者TCP_SEND_QUEUE，然后（2）调用recvmesg()来读取(1)中选取队列的内容。</p>
<p>事实上在用户空间只有一种重要的信息获取不到：两端在连接建立时协商后确定的MSS的最大值。为了取到这个值，Pavel的patch改变了处于维修模式下时，socket选项TCP_MAXSEG的语义，它将返回最大的”clamp” MSS值，而不再是当前正在使用的值。</p>
<p>最后，如果一个连接在它处于维修模式的时候结束了，那么它将被简单的删除掉，而不会给对端任何的通知。也就是说FIN包和RST包都不会发，因此对端将对情况的变化一无所知。</p>
<p>然后就是到新主机上建立连接的事了，这是通过在新主机上新建一个socket并且将其立即置入修复模式实现的。这个socket之后可以被绑定到合适的端口上，处于修复模式的时候，对于端口号进行的寻常检查将被暂停执行。</p>
<p>接下来又要用setsockopt设置TCP_REPAIR_QUEUE选项了，不过这次之后会调用sendmsg来restore发送和接收队列的内容。</p>
<p>另一个重要的任务是restore发送和接收的sequence number。这些sequence number通常是在连接建立时随机生成的，不过在连接迁移时不能那样做。这些sequence number通过调用setsockopt，设置TCP_QUEUE_SEQ选项来设置，队列参数的选取和使用TCP_REPAIR_QUEUE选项时一样，这样一来发送和接受队列的内容和sequence number的设定就被很好的恢复了。</p>
<p>一些协商而来的参数也需要被restore，这样两端才能彼此继续维持之前的agreement。这其中包括前面提到的MSS clamp，以及当前的MSS，window size和SACK以及TIMESTAMP是否可用的设置。最后一个新增选项TCP_REPAIR_OPTIONS，就是被添加来在用户态设置这些参数的。</p>
<p>一旦socket被恢复到了和其在旧主机上差不多的状态，就到了投入使用的时候了。当一个socket处于修复模式下时调用connect()，连接建立和协商的大部分代码会被跳过，连接会直接转到“established”状态，而不会与对端发生任何的实质通信。最后，当socket退出修复模式的时候，会发送一个window probe，以重启两端的traffic，此时socket可以在新的主机上继续正常的通信操作。</p>
<p>这些patch在几个月里已经经过了一些修订，到version 4的时候，networking的maintainer David Miller已经把它们接受到了net-next中。因此，这些改动几乎肯定将被merge进3.5版中。TCP connection repair patch并没有提供container checkpointing和restoring问题的完整solution，不过它们是这个方向上的重要一步。</p>
<p>以下是原文：</p>
<blockquote>
<h1 id="TCP-connection-repair"><a href="#TCP-connection-repair" class="headerlink" title="TCP connection repair"></a>TCP connection repair</h1><p> Migrating a running container from one physical host to another is a tricky job on a number of levels. Things get even harder if, as is likely, the container has active network connections to processes outside of that container. It is natural to want those connections to follow the container to its new host, preferably without the remote end even noticing that something has changed, but the Linux networking stack was not written with this kind of move in mind. Even so, it appears that transparent relocation of network connections, in the form of Pavel Emelyanov’s TCP connection repair patches, will be supported in the 3.5 kernel. </p>
<p> The first step in moving a TCP connection is to gather all of the information possible about its current state. Much of that information is available from user space now; by digging around in /proc and /sys, one can determine the address and port of the remote end, the sizes of the send and receive queues, TCP sequence numbers, and a number of parameters negotiated between the two end points. There are still a few things that user space will need to obtain, though, before it can finish the job; that requires some additional support from the kernel. </p>
<p> With Pavel’s patch, that support is available to suitably privileged processes. To dig into the internals of an active network connection, user space must put the associated socket into a new “repair mode.” That is done with the setsockopt() system call, using the new TCP_REPAIR option. Changing a process’s repair mode status requires the CAP_NET_ADMIN capability; the socket must also either be closed or in the “established” state. Once the socket is in repair mode, it can be manipulated in a number of ways.</p>
<p> One of those is to read the contents of the send and receive queues. The send queue contains data that has not yet been successfully transmitted to the remote end; that data needs to move with the connection so it can be transmitted from the new location. The receive queue, instead, contains data received from the remote end that has not yet been consumed by the application being moved; that data, too, should move so it will be waiting on the new host when the application gets around to reading it. Obtaining the contents of these queues is done with a two-step sequence: (1) call setsockopt(TCP_REPAIR_QUEUE) with either TCP_RECV_QUEUE or TCP_SEND_QUEUE, then (2) call recvmesg() to read the contents of the selected queue.</p>
<p> It turns out there is only one other important piece of information that cannot already be obtained from user space: the maximum value of the MSS (maximum segment size) negotiated between the two endpoints at connection setup time. To make this value available, Pavel’s patch changes the semantics of the TCP_MAXSEG socket option (for getsockopt()) when the connection is in repair mode: it returns the maximal “clamp” MSS value rather than the currently active value.</p>
<p> Finally, if a connection is closed while it is in the repair mode, it is simply deleted with no notification to the remote end. No FIN or RST packets will be sent, so the remote side will have no idea that things have changed.</p>
<p> Then there is the matter of establishing the connection on the new host. That is done by creating a new socket and putting it immediately into the repair mode. The socket can then be bound to the proper port number; a number of the usual checks for port numbers are suspended when the socket is in repair mode. </p>
<p> The TCP_REPAIR_QUEUE setsockopt() call comes into play again, but this time sendmsg() is used to restore the contents of the send and receive queues.</p>
<p> A few negotiated parameters also need to be restored so that the two ends will remain in agreement with each other; these include the MSS clamp described above, along with the active maximum segment size, the window size, and whether the selective acknowledgment and timestamp features can be used. One last setsockopt() option, TCP_REPAIR_OPTIONS, has been added to make it possible to set these parameters from user space.</p>
<p> Another important task is to restore the send and receive sequence numbers. These numbers are normally generated randomly when the connection is established, but that cannot be done when a connection is being moved. These numbers can be set with yet another call to setsockopt(), this time with the TCP_QUEUE_SEQ option. This operation applies to whichever queue was previously selected with TCP_REPAIR_QUEUE, so the refilling of a queue’s content and the setting of its sequence number are best done at the same time.</p>
<p> Once the socket has been restored to a state approximating that which existed on the old host, it’s time to put it into operation. Whenconnect() is called on a socket in repair mode, much of the current setup and negotiation code is shorted out; instead, the connection goes directly to the “established” state without any communication from the remote end. As a final step, when the socket is taken out of the repair mode, a window probe is sent to restart traffic between the two ends; at that point, the socket can resume normal operation on the new host.</p>
<p> These patches have been through a few revisions over a number of months; with version 4, networking maintainer David Miller accepted them into net-next. From there, those changes will almost certainly hit the mainline during the 3.5 merge window. The TCP connection repair patches do not represent a complete solution to the problem of checkpointing and restoring containers, but they are an important step in that direction.</p>
</blockquote>
<p><a href="https://lwn.net/Articles/495304/" target="_blank" rel="external">https://lwn.net/Articles/495304/</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
  var gitment = new Gitment({
    owner: 'efunflying',
    repo: 'comments',
    oauth: {
      id: 'repair-tcp-connection',
      client_id: 'd4f8ceb2a0018f4659f4',
      client_secret: 'f1b0d2899c3e538b720bd7ede322d15faca22e13',
    },
  })
  gitment.render('container')
</script>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是2012年的一篇老文章了，最近在做产品kernel base upgrading的事，从2.6.32到3.10的迁移过程中，原来在kernel中实现man in middle支持的patch在porting到3.10后出了一点问题，为maninmid建立的新socket
    
    </summary>
    
    
      <category term="kernel" scheme="https://efunflying.github.io/tags/kernel/"/>
    
      <category term="TCP/IP" scheme="https://efunflying.github.io/tags/TCP-IP/"/>
    
      <category term="lwn.net" scheme="https://efunflying.github.io/tags/lwn-net/"/>
    
  </entry>
  
  <entry>
    <title>一道矩阵转置的算法题</title>
    <link href="https://efunflying.github.io/2017/07/17/algorith-reverse-matrix/"/>
    <id>https://efunflying.github.io/2017/07/17/algorith-reverse-matrix/</id>
    <published>2017-07-17T15:45:57.000Z</published>
    <updated>2017-07-17T17:59:54.000Z</updated>
    
    <content type="html"><![CDATA[<p>今天和新来的同事闲聊中听到一道算法题，问一个2*5的矩阵，怎样原地转置。回来简单想了一下，画了一下图之后，思路一下就打开了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">---------</div><div class="line">| 0 | 1 |</div><div class="line">---------</div><div class="line">| 2 | 3 |                       ---------------------</div><div class="line">---------                       | 0 | 2 | 4 | 6 | 8 |</div><div class="line">| 4 | 5 |         ===&gt;          ---------------------</div><div class="line">---------                       | 1 | 3 | 5 | 7 | 9 |</div><div class="line">| 6 | 7 |                       ---------------------</div><div class="line">---------                              (2)           </div><div class="line">| 8 | 9 |</div><div class="line">---------</div><div class="line">  (1)</div></pre></td></tr></table></figure>
<p>(2)把图画成这样，就更清楚了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">                                  ---------</div><div class="line">                                  | 0 | 2 |</div><div class="line">                                  ---------</div><div class="line">---------------------             | 4 | 6 |</div><div class="line">| 0 | 2 | 4 | 6 | 8 |             ---------</div><div class="line">---------------------    ===&gt;     | 8 | 1 |</div><div class="line">| 1 | 3 | 5 | 7 | 9 |             ---------</div><div class="line">---------------------             | 3 | 5 |</div><div class="line">       (2)                        ---------</div><div class="line">                                  | 7 | 9 |</div><div class="line">                                  ---------</div><div class="line">                                     (2&apos;)</div></pre></td></tr></table></figure>
<p>看起来有这样的关系：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">假设原位置是k，则</div><div class="line">if (k == 2 * i)</div><div class="line">&#123;</div><div class="line">    target(k) = i;</div><div class="line">&#125; </div><div class="line">if (k == 2 * i + 1)</div><div class="line">&#123;</div><div class="line">    target(k) = 5 + i;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注意到每个轮转最后都是一个闭环(应该是可以证明的):<br>以i=0为例，2*0+1=1, target(1)=5; target(5)=7; target(7)=8; target(8)=4; target(4)=2; target(2)=1;</p>
<p>因此有：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">count = 10</div><div class="line">count = count - 2 /*首尾两单元不用动*/</div><div class="line">for(i=0; 2*i+1&lt;5 &amp;&amp; count &gt;0; i++)</div><div class="line">&#123;</div><div class="line">    transfer(2 * i + 1);</div><div class="line">&#125;</div><div class="line"></div><div class="line">transfer(k)</div><div class="line">&#123;</div><div class="line">    for(i = k; k!=target(i); i=target(i))</div><div class="line">    &#123;</div><div class="line">        tmp = array[target(i)];</div><div class="line">        array[target(i)] = tmp;</div><div class="line">        count --; /*某个元素已经就位*/</div><div class="line">    &#125;</div><div class="line">    array[k] = tmp;</div><div class="line">    count --; /*某个元素已经就位*/</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>因此最终只需要移动10-2次，一个单位的临时存储就可以完成。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天和新来的同事闲聊中听到一道算法题，问一个2*5的矩阵，怎样原地转置。回来简单想了一下，画了一下图之后，思路一下就打开了。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;di
    
    </summary>
    
    
      <category term="Algorithm" scheme="https://efunflying.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>利用SystemTap辅助查找kernel panic的原因</title>
    <link href="https://efunflying.github.io/2017/07/09/system-tap-kernel-update/"/>
    <id>https://efunflying.github.io/2017/07/09/system-tap-kernel-update/</id>
    <published>2017-07-09T12:42:55.000Z</published>
    <updated>2017-07-09T12:54:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在把产品kernel base从centos 6携带的2.6.32升级到centos 7携带的3.10的过程中，遇到kernel panic的case，问题可重现，且产生的vmcore现场都非常相似，判断应该是同一个问题：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">PID: 7220   TASK: ffff88006580bec0  CPU: 1   COMMAND: &quot;dpid&quot;  </div><div class="line"> #0 [ffff8800658c7b30] machine_kexec at ffffffff81059bdb  </div><div class="line"> #1 [ffff8800658c7b90] __crash_kexec at ffffffff81105382</div><div class="line"> #2 [ffff8800658c7c60] crash_kexec at ffffffff81105470</div><div class="line"> #3 [ffff8800658c7c78] oops_end at ffffffff8168cd88</div><div class="line"> #4 [ffff8800658c7ca0] die at ffffffff8102e93b</div><div class="line"> #5 [ffff8800658c7cd0] do_trap at ffffffff8168c440</div><div class="line"> #6 [ffff8800658c7d20] do_divide_error at ffffffff8102af4e</div><div class="line"> #7 [ffff8800658c7dd0] divide_error at ffffffff81695cce</div><div class="line">    [exception RIP: tcp_select_initial_window+55]</div><div class="line">    RIP: ffffffff815c9e77  RSP: ffff8800658c7e80  RFLAGS: 00010246</div><div class="line">    RAX: 000000000000aaaa  RBX: 0000000000000000  RCX: 000000003fffc000</div><div class="line">    RDX: 0000000000000000  RSI: 0000000000000000  RDI: 000000000000aaaa</div><div class="line">    RBP: ffff8800658c7e88   R8: 0000000000000001   R9: ffff8800658c7eab</div><div class="line">    R10: ffff880138ea4e18  R12: ffff880138ea4eac  R12: ffff880135087c00</div><div class="line">    R13: 0000000000000000  R14: 00000000650cb5dc  R15: ffffffff81a97e40</div><div class="line">    ORIG_RAX: ffffffffffffffff  CS: 0010  SS: 0018</div><div class="line"> #8 [ffff8800658c7e90] do_tcp_setsockopt at ffffffff815be132</div><div class="line"> #9 [ffff8800658c7f18] tcp_setsockopt at ffffffff815be9b2</div><div class="line">#10 [ffff8800658c7f28] sock_common_setsockopt at ffffffff81554384</div><div class="line">#11 [ffff8800658c7f38] sys_setsockopt at ffffffff81553510</div><div class="line">#12 [ffff8800658c7f80] system_call_fastpath at ffffffff816944c9</div><div class="line">    RIP: 00007f0f4a248c0a  RSP: 00007f0ee3ffe188  RFLAGS: 00010202</div><div class="line">    RAX: 0000000000000036  RBX: ffffffff816944c9  RCX: 00007f0ee3ffe0b0</div><div class="line">    RDX: 0000000000000014  RSI: 0000000000000006  RDI: 000000000000005a</div><div class="line">    RBP: 00007f0ee3ffe180   R8: 0000000000000022   R9: 00007f0ee3ffe6ec</div><div class="line">    R10: 00007f0ee3ffe100  R11: 0000000000000246  R12: 00007f0f50490860</div><div class="line">    R13: 00007f0ee3ffe7f0  R14: 0000000000000000  R15: 000000000046a24a</div><div class="line">    ORIG_RAX: 0000000000000036  CS: 0033  SS: 002b</div></pre></td></tr></table></figure>
<p>根据调用栈，锁定这是一段这样的代码导致（这段代码来自从老kernel上porting过来的一个功能patch）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"> 93 +   sync_mss = tcp_sync_mss(sk, dst_mtu(dst));</div><div class="line"> 94 +</div><div class="line"> 95 +   tp-&gt;window_clamp = dst_metric(dst, RTAX_WINDOW);</div><div class="line"> 96 +   tp-&gt;advmss = dst_metric(dst, RTAX_ADVMSS);</div><div class="line"> 97 +</div><div class="line"> 98 +   tcp_initialize_rcv_mss(sk);</div><div class="line"> 99 +   tcp_select_initial_window(tcp_full_space(sk),</div><div class="line">100 +                 tp-&gt;advmss - (tp-&gt;rx_opt.ts_recent_stamp ? tp-&gt;tcp_header_len - sizeof(struct tcphdr) : 0),</div><div class="line">101 +                 &amp;tp-&gt;rcv_wnd,</div><div class="line">102 +                 &amp;tp-&gt;window_clamp,</div><div class="line">104 +                 sysctl_tcp_window_scaling,</div><div class="line">104 +                 &amp;rcv_wscale, dst_metric(dst, RTAX_INITRWND));</div></pre></td></tr></table></figure>
<p>由64位下参数调用的习惯，可以得知100行处tcp_select_initial_window的第二个参数mss值为0, 此参数会在tcp_select_initial_window中作为除数，从而引发一个除零异常。</p>
<p>这时以前比较常用的做法是通过bt -lf打出栈内存，反汇编问题函数，然后通过推断得出各个变量的地址，然后根据类型去解相应的值，重建问题的现场。但这里的麻烦之处在于编译的优化常常会出来捣乱，让代码变得更加难读，更糟的情况是dump信息中有时并不能包含所有的内存地址，这会导致推理中断。</p>
<p>所幸的是这个问题是可以重现的，因此一个比较直观的思路是可以打log出来看一下到底那些中间变量发生了什么，如果用修改代码来实施的话，每次想收集新的信息时就得重新编译，外加更换产品的内核，很麻烦。</p>
<p>这时候SystemTap就显得非常好用，直接probe一下出问题点前的程序步，打印出相应的变量值就可以了。不过这里有个特殊的问题是这个bug的后果会导致kernel panic，因此现场信息无法及时的返回回来，所以用了个小trick，在99行调用前除了打印变量值外，把tp-&gt;advmss的值调大，这样就可以避过kernel panic，看到现场变量的值了。结果发现tp-&gt;advmess的值在函数调用时是0，tp-&gt;rx_opt.ts_recent_stamp也是0，根源在tp-&gt;advmss上。</p>
<p>通过查看kernel 3.10的代码，发现dst_metric(dst, RTAX_ADVMSS)这种方式在新的kernel中已经不再工作了，新的取值方式是dst_metric_advmss(dst); 修改为新的方式，重新build kernel，问题解决。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在把产品kernel base从centos 6携带的2.6.32升级到centos 7携带的3.10的过程中，遇到kernel panic的case，问题可重现，且产生的vmcore现场都非常相似，判断应该是同一个问题：&lt;/p&gt;
&lt;figure class=&quot;high
    
    </summary>
    
    
      <category term="kernel" scheme="https://efunflying.github.io/tags/kernel/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning课程感想</title>
    <link href="https://efunflying.github.io/2017/06/25/Machine-Learning-Start/"/>
    <id>https://efunflying.github.io/2017/06/25/Machine-Learning-Start/</id>
    <published>2017-06-25T04:57:01.000Z</published>
    <updated>2017-06-30T07:00:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近机器学习真是火，百年难得一见的创始人，为此竟然跑出来给大家做了次sharing，虽然内容还是很general，但是从创始人的口中了解到现在在硅谷，AI方面的购并正在大规模的发生，各大公司正在摩拳擦掌准备抢占新时代的主导权，或者至少，为应对新一轮的“洗牌”积蓄力量，以免自己成为被新浪潮抛弃掉的那一批。</p>
<p>有鉴于IT行业从来不缺乏炒概念，讲故事的热情和能力，因此对这次的AI风潮，一直也都是抱着隔岸观火的态度。毕竟根据在学校里那会儿数据挖掘课上得到的印象，机器学习还只是基于算法的一种辅助寻找大数据中有用信息和规律的手段而已，能力受限于设计者对问题的认识比较多，完全谈不上什么超越人类，取代人类。即使是Alpha Go的神奇，初看起来也只是由于剪枝做得比较好，节省了许多原本不必要的计算，外加加入了形势判断的算法，能去计算落子的效率和优劣而已。</p>
<p>不过另一方面也深知，要客观评价一个东西，得先足够了解它才行，记得上学那会儿数据挖掘课上提到神经网络的时候，只是简单的讲了一下，印象最深的还是Minsky大神在他的那本《感知机》的书中对于神经网络的数学论证，导致神经网络研究直接进入寒冬的掌故，神经网络的课也主要讲的是BP神经网络，还讲到了SVM，那个又一次把神经网络研究打入冷宫的算法，但之于当红的Deep Learning，那会儿是没有讲到的。所以正好公司内部有这样的课程就报名参加了。</p>
<p>听了几堂课下来，前面的基础知识的内容和研究生时期的课差不多，不过由于是公司的课，更偏工程方向。后面讲到深度学习的时候，才了解到目前这个方向更类似于实验科学，大家对这项技术的热衷源于它的实际效果很好，并且通过增加神经网络的深度，调试激活函数，还能取得更好的效果，并且与现有的其他模型不同，深度学习目前并没有表现出传统机器学习算法那样随着数据的增加效用逐渐趋平的天花板效应。课间和讲师同事闲聊的时候，他也坦言，现在的这个方向，理论严重滞后于实践，大家的工作更像是炼金术士，追求的是怎样可以取得更好的效果，但是对于为什么这样那样效果更好，或者是不是总是会那么好，其实大家都不清楚。</p>
<p>而这一点倒是让我感到很惊艳，因为这像极了许多科学理论大发展前的实践积累阶段，而历史也反复证明，一旦进入理论化的阶段，其发展可能是爆炸式的。而且这一次有意思的地方在于，由于神经网络最初是模拟人脑神经元的工作方式而诞生的，那么将来这方面一旦理论化，也许也意味着我们能找到对人意识思维本质这一问题的答案，并且从这个意义上讲，一旦那一天到来，机器由于其永生，外加算力强大，超越人类简直易如反掌。这也让我第一次感到人工智能也许真在某个时间点后，将完全脱离人的掌握而独立存在这种可能的现实性。</p>
<p>炼金到化学的跨越，也许也是人到神的跨越。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近机器学习真是火，百年难得一见的创始人，为此竟然跑出来给大家做了次sharing，虽然内容还是很general，但是从创始人的口中了解到现在在硅谷，AI方面的购并正在大规模的发生，各大公司正在摩拳擦掌准备抢占新时代的主导权，或者至少，为应对新一轮的“洗牌”积蓄力量，以免自
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>一些很有用的RPM命令的总结</title>
    <link href="https://efunflying.github.io/2017/04/12/rpm-commands/"/>
    <id>https://efunflying.github.io/2017/04/12/rpm-commands/</id>
    <published>2017-04-12T05:36:05.000Z</published>
    <updated>2017-07-02T05:49:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近做一个项目，目标是把产品定制操作系统的kernel base从CentOS 6的2.6.32升级到CentOS 7的3.10，同时升级操作系统编译平台，使其可以编译出新的定制kernel包。同时希望维持产品其余部分的稳定。</p>
<p>简言之是希望把CentOS 7的kernel包加上定制patch后装到CentOS 6上，这是一个并没有官方guide的task，由于kernel的升级，不可避免的会带来toolchain的升级需求，因此在很多时候，是需要解决在过程中的包依赖（包括版本依赖）的，也就不可避免的要做一些包的backport，以使新kernel的rpm包依赖的一些rpm包能运行在CentOS 6下。</p>
<p>在这个过程中，一些命令发现被用到的频率非常高，所以在这里整理一下。</p>
<ol>
<li>rpm -qpl xxxxx.rpm     有时候下了一个包，你想知道这个包里有哪些文件，将来会被放到什么目录下，这个命令很有用</li>
<li>rpm -qf /path/to/file  有时候提示你CentOS 6下缺了某个文件，你想知道这个文件应该去CentOS 7下的哪个包里找，可以在CentOS 7下用whereis找到该文件，然后用这个命令查出属于哪个包</li>
<li>rpm2cpio xxxxx.rpm | cpio -dim  当你想查看一下rpm包内的某个文件，但却并不想安装它的时候，把它解开来是个比较简单的方法</li>
<li>rpm -qp –scripts xxxx.rpm 当你想查看一下某个rpm包在安装前后会执行哪些操作时，可以用这个命令来看，在安装过程（rpm -i）中加-vv参数可以观察这些脚本的执行情况。</li>
<li>rpm -qpR xxxxx.rpm     有时候你想提前知道某个包依赖哪些包，可以通过这个命令查看</li>
<li>rpm -q –whatrequires xxxx  查看某包被哪些包依赖</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近做一个项目，目标是把产品定制操作系统的kernel base从CentOS 6的2.6.32升级到CentOS 7的3.10，同时升级操作系统编译平台，使其可以编译出新的定制kernel包。同时希望维持产品其余部分的稳定。&lt;/p&gt;
&lt;p&gt;简言之是希望把CentOS 7的
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://efunflying.github.io/2015/08/22/2015-08-22-hello-world/"/>
    <id>https://efunflying.github.io/2015/08/22/2015-08-22-hello-world/</id>
    <published>2015-08-21T19:22:00.000Z</published>
    <updated>2017-07-19T19:34:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
<p><div id="container"></div></p>
<p><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"></p>
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script>
  var gitment = new Gitment({
    owner: 'efunflying',
    repo: 'comments',
    oauth: {
      id: 'hello-world',
      client_id: 'd4f8ceb2a0018f4659f4',
      client_secret: 'f1b0d2899c3e538b720bd7ede322d15faca22e13',
    },
  })
  gitment.render('container')
</script>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
